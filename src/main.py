import streamlit as st
from database_vector import create_ChromaDB
from chain_retrieval import create_chain
from dotenv import load_dotenv
from langchain_core.messages import HumanMessage

def load_api_keys():
    load_dotenv()

def setup_page_config():
    st.set_page_config(
        page_title="ChatBot_PDF",  
        page_icon="üí¨",
        
    )

def handle_file(embedd_model):
    st.header("T·∫£i t·ªáp PDF ")
    pdf_file = st.file_uploader(label="Upload pdf here", type= ["pdf"])

    if "bool_db" not in st.session_state:
        st.session_state.bool_db = False
    
    if pdf_file is not None:
        #l·∫•y t√™n file hi·ªán t·∫°i
        current_filename = pdf_file.name     
        # ki·ªÉm tra xem c√≥ thay ƒë·ªïi file kh√¥ng
        if "upload_file" in st.session_state:
            if current_filename != st.session_state.upload_file:
                st.session_state.bool_db = False
        else :
            st.session_state.upload_file = current_filename

        if st.session_state.bool_db == False:
            with st.spinner("Vui l√≤ng ch·ªù trong gi√¢y l√°t"):
                try:

                    vector_store = create_ChromaDB(pdf_file=pdf_file, embed_model=embedd_model)
                    st.session_state.bool_db = True
                    st.success("T·∫°o vectorDB th√†nh c√¥ng.")
                except Exception as e:
                    st.error(f" l·ªói {str(e)}")
        # pass
        
    else:
        st.session_state.bool_db = False
        st.warning("B·∫°n c·∫ßn t·∫£i √≠t nh·∫•t m·ªôt file PDF tr∆∞·ªõc khi chat.")
        st.stop()  # L·ªánh n√†y s·∫Ω d·ª´ng c√°c ph·∫ßn code b√™n d∆∞·ªõi, ·∫©n lu√¥n giao di·ªán chat
    


def setup_sidebar():
    with st.sidebar:
        st.title("Chatbot Setting")
        
        # ch·ªçn model embed
        embedd_model = st.sidebar.selectbox(label="L·ª±a ch·ªçn model embedding", 
                                            options=["nomic-embed-text", "text-embedding-3-large"])
        if embedd_model == "llama3.1":
            print("Model chat: ", embedd_model)
        else: # text-embedding-3-large
            print("model embedding: ", embedd_model)
        # x√≥a l·ªãch s·ª≠ chat
        if st.button(label="Clear history"):
            st.session_state.message = []
            print("Delete history")
           
        # ch·ªçn model chat
        chat_model = st.sidebar.selectbox(label="L·ª±a ch·ªçn model chat", options=["llama3.1", "gpt-4o-mini"])
        if chat_model == "llama3.1":
            chain = create_chain(chat_model=chat_model, embedd_model=embedd_model)
            print("Model chat: ",chat_model)
        else: # gpt4o-mini
            chain = create_chain(chat_model=chat_model, embedd_model=embedd_model)
            print("model chat: ", chat_model)

    return embedd_model,chain


def display_chat_history():
    st.title("Asisstant ChatBot")
    # kh·ªüi t·∫°o chat history
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # hi·ªÉn th·ªã tin nh·∫Øn tr√≤ chuy·ªán t·ª´ l·ªãch s·ª≠ khi ch·∫°y l·∫°i ·ª©ng d·ª•ng
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.write(message["content"])


def handle_user_input(llm_with_tools):

    # ƒë·∫ßu v√†o ng∆∞·ªùi d√πng
    if prompt:=st.chat_input(placeholder="Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n ·ªü ƒë√¢y?"):
        # th√™m tin nh·∫Øn ng∆∞·ªùi d√πng v√†o l·ªãch s·ª≠ chat
        st.session_state.messages.append({"role": "user", "content": prompt})
        # hi·ªÉn th·ªã tin nh·∫Øn ng∆∞·ªùi d√πng trong chat message container
        with st.chat_message("user"):
            st.write(prompt)
        
        # hi·ªÉn th·ªã ph·∫£n h·ªìi c·ªßa ai trong chat message container
        with st.chat_message("assistant"):
            # l·∫•y l·ªãch s·ª≠ chat l∆∞u trong session_state
            chat_history = []
            for message in st.session_state.messages[:-1]:
                chat_history.append({"role": message["role"], "content": message["content"]})
            # l·∫•y ph·∫£n h·ªìi t·ª´ chain
            # response = chain.invoke({
            #     "input": prompt,
            #     "chat_history": chat_history    
            #     }
            # )
            reponse = llm_with_tools.invoke([HumanMessage(content=prompt)])
            print("chat history: \n", chat_history)
            st.write(reponse)
        # Add assistant response to chat history
        st.session_state.messages.append({"role": "assistant", "content": output})
        


def main():

    load_api_keys()
    setup_page_config()
    embedd_model, llm_with_tool = setup_sidebar()
    handle_file(embedd_model)
    display_chat_history()
    handle_user_input(llm_with_tool)
    print("#######################")


if __name__ == "__main__":
    main()

